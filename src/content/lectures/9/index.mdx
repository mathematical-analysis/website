---
title: "Partial derivatives. Gradient descent"
description: "Lorem ipsum dolor sit amet"
pubDate: "Jul 18 2022"
---

Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent.

The algorithm is given by the following steps:

- 1. Pick a starting point $ x_0 $.
- 2. Compute the gradient $ \nabla f(x) $.
- 3. Update the current point by taking a step in the opposite direction of the gradient: $ x\_{k+1} = x_k - \alpha \nabla f(x_k) $, where $ \alpha $ is the step size.
- 4. Repeat steps 2 and 3 until the algorithm converges.

The algorithm is guaranteed to converge to a local minimum under certain conditions on the step size $ \alpha $ and the function $ f $.

Python gradient descent:

```py
import torch

def gradient_descent(f, x0, alpha, max_iter):
    x = x0
    for i in range(max_iter):
        grad = torch.autograd.grad(f(x), x)[0]
        x = x - alpha * grad
    return x
```
